{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome\n\n\nThis site is dedicated to the documentation of the recipes brought by\n\nSmartSDK\n to launch, test and experiment different\n\nFIWARE Generic Enablers\n and combinations of them known as common use cases.\n\n\nRecipes will be based on \nDocker\n, \ndocker-compose\n and maybe some complementary scripts. For simple cases, they will be aimed to run on a single Docker host. For more complex scenarios, they will be aimed to run in a \nDocker Swarm\n.\n\n\nRecipes will be organized in folders respecting the FIWARE chapters and generic enablers they are mainly aimed to work with.\n\n\nGetting started\n\n\nHead to \nhttps://github.com/martel-innovate/smartsdk-recipes\n and follow the instructions to get the code and setup your environment.\n\n\nAlso, have a look at the \ntools\n section, specially if you want to test the recipes in a local environment or want to contribute.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome", 
            "text": "This site is dedicated to the documentation of the recipes brought by SmartSDK  to launch, test and experiment different FIWARE Generic Enablers  and combinations of them known as common use cases.  Recipes will be based on  Docker ,  docker-compose  and maybe some complementary scripts. For simple cases, they will be aimed to run on a single Docker host. For more complex scenarios, they will be aimed to run in a  Docker Swarm .  Recipes will be organized in folders respecting the FIWARE chapters and generic enablers they are mainly aimed to work with.", 
            "title": "Welcome"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Head to  https://github.com/martel-innovate/smartsdk-recipes  and follow the instructions to get the code and setup your environment.  Also, have a look at the  tools  section, specially if you want to test the recipes in a local environment or want to contribute.", 
            "title": "Getting started"
        }, 
        {
            "location": "/data-management/context-broker/readme/", 
            "text": "Orion\n\n\nHere you can find recipes aimed at different usages of the Orion Context Broker. We assume you are already familiar with Orion. If not, refer to the \nofficial documentation\n.\n\n\nThe easiest and simplest way to try Orion is as explained in \nOrion's official docker image docs\n using  \nthis docker-compose\n file. But here, we will explore a distributed configuration for this Generic Enabler.\n\n\nIf you are new to docker as well you probably want to start looking at the \nsimple recipe\n to have a first view of how a single-host simple scenario looks like.\n\n\nInstructions on how to prepare your environment to test these recipes are given in \nhttps://github.com/martel-innovate/smartsdk-recipes\n.", 
            "title": "Intro"
        }, 
        {
            "location": "/data-management/context-broker/readme/#orion", 
            "text": "Here you can find recipes aimed at different usages of the Orion Context Broker. We assume you are already familiar with Orion. If not, refer to the  official documentation .  The easiest and simplest way to try Orion is as explained in  Orion's official docker image docs  using   this docker-compose  file. But here, we will explore a distributed configuration for this Generic Enabler.  If you are new to docker as well you probably want to start looking at the  simple recipe  to have a first view of how a single-host simple scenario looks like.  Instructions on how to prepare your environment to test these recipes are given in  https://github.com/martel-innovate/smartsdk-recipes .", 
            "title": "Orion"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/", 
            "text": "Single Host Scenario\n\n\nIntroduction\n\n\nThis simple recipe triggers an \nOrion Context Broker\n instance backed with a \nMongoDB\n instance everything running \non an single host\n.\n\n\n\n\nBoth services will be running in docker containers, defined in the \n./docker-compose.yml\n file.\n\n\nData will be persisted in a local folder defined by the value of \nDATA_PATH\n variable in the \n.env\n file.\n\n\nHow to use\n\n\nThis recipes has some default values, but optionally you can explore different configurations by modifying the \n.env\n file, the \ndocker-compose.yml\n or even the \nscripts/setup.sh\n.\n\n\nThen, from this folder simply run:\n\n\n$ docker-compose up -d\n\n\n\nHow to validate\n\n\nBefore testing make sure docker finished downloading the images and spinning-off the containers. You can check that by running:\n\n\n$ docker ps\n\n\n\nYou should see the two containers listed and with status \"up\".\n\n\nThen, to test if orion is truly up and running run:\n\n\n$ sh ../query.sh\n\n\n\nIt should return something like:\n\n\n{\n\"orion\" : {\n  \"version\" : \"1.6.0-next\",\n  \"uptime\" : \"0 d, 0 h, 5 m, 24 s\",\n  \"git_hash\" : \"61be6c26c59469621a664d7aeb1490d6363cad38\",\n  \"compile_time\" : \"Tue Jan 24 10:52:30 UTC 2017\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"b99744612d0b\"\n}\n}\n[]", 
            "title": "Simple"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/#single-host-scenario", 
            "text": "", 
            "title": "Single Host Scenario"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/#introduction", 
            "text": "This simple recipe triggers an  Orion Context Broker  instance backed with a  MongoDB  instance everything running  on an single host .   Both services will be running in docker containers, defined in the  ./docker-compose.yml  file.  Data will be persisted in a local folder defined by the value of  DATA_PATH  variable in the  .env  file.", 
            "title": "Introduction"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/#how-to-use", 
            "text": "This recipes has some default values, but optionally you can explore different configurations by modifying the  .env  file, the  docker-compose.yml  or even the  scripts/setup.sh .  Then, from this folder simply run:  $ docker-compose up -d", 
            "title": "How to use"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/#how-to-validate", 
            "text": "Before testing make sure docker finished downloading the images and spinning-off the containers. You can check that by running:  $ docker ps  You should see the two containers listed and with status \"up\".  Then, to test if orion is truly up and running run:  $ sh ../query.sh  It should return something like:  {\n\"orion\" : {\n  \"version\" : \"1.6.0-next\",\n  \"uptime\" : \"0 d, 0 h, 5 m, 24 s\",\n  \"git_hash\" : \"61be6c26c59469621a664d7aeb1490d6363cad38\",\n  \"compile_time\" : \"Tue Jan 24 10:52:30 UTC 2017\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"b99744612d0b\"\n}\n}\n[]", 
            "title": "How to validate"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/", 
            "text": "Orion in HA\n\n\nThis recipe shows how to deploy an scalable \nOrion Context Broker\n service backed with an scalable \nreplica set\n of MongoDB instances.\n\n\nAll elements will be running in docker containers, defined in docker-compose files. Actually, this recipe focuses on the deployment of the Orion frontend, reusing the \nmongodb replica recipe\n for its backend.\n\n\nThe final deployment is represented by the following picture:\n\n\n\n\nIMPORTANT:\n As explained in the \nmongo replica recipe\n, that recipe is not ready for production deployments for security reasons. Look at the \n\"Further Improvements\"\n section for more details.\n\n\nHow to use\n\n\nFirstly, you need to have a Docker Swarm (docker \n= 1.13) already setup. If you don't have one, checkout the \ntools\n section for a quick way to setup a local swarm.\n\n\n$ miniswarm start 3\n$ eval $(docker-machine env ms-manager0)\n\n\n\nThen, simply run...\n\n\n$ sh deploy_back.sh\n\n\n\nWait some time until the backend is ready and then...\n\n\n$ sh deploy_front.sh\n\n\n\nAt some point, your deployment should look like this...\n\n\n$ docker service ls\nID            NAME                            MODE        REPLICAS  IMAGE\nnrxbm6k0a2yn  orion-backend_mongo             global      3/3       mongo:3.2\nrgws8vumqye2  orion-backend_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\nzk7nu592vsde  orion_orion                     replicated  3/3       fiware/orion:1.3.0\n\n\n\nAs shown above, if you see \n3/3\n in the replicas column it means the 3 replicas are up and running.\n\n\nA walkthrough\n\n\nYou can check the distribution of the containers of a service (a.k.a tasks) through the swarm running the following...\n\n\n$ docker service ps orion_orion\nID            NAME                    IMAGE               NODE         DESIRED STATE  CURRENT STATE               ERROR  PORTS\nwwgt3q6nqqg3  orion_orion.1  fiware/orion:1.3.0  ms-worker0   Running        Running 9 minutes ago          \nl1wavgqra8ry  orion_orion.2  fiware/orion:1.3.0  ms-worker1   Running        Running 9 minutes ago          \nz20v0pnym8ky  orion_orion.3  fiware/orion:1.3.0  ms-manager0  Running        Running 25 minutes ago\n\n\n\nThe good news is that, as you can see from the above output, by default docker already took care of deploying all the replicas of the service \ncontext-broker_orion\n to different hosts.\n\n\nOf course, with the use of labels, constraints or deploying mode you have the power to customize the distribution of tasks among swarm nodes. You can see the \nmongo replica recipe\n to understand the deployment of the \nmongo-replica_mongo\n service.\n\n\nNow, let's query Orion to check it's truly up and running. The question now is... where is Orion actually running? We'll cover the network internals later, but for now let's query the manager node...\n\n\n$ sh ../query.sh $(docker-machine ip ms-manager0)\n\n\n\nYou will get something like...\n\n\n{\n  \"orion\" : {\n  \"version\" : \"1.3.0\",\n  \"uptime\" : \"0 d, 0 h, 18 m, 13 s\",\n  \"git_hash\" : \"cb6813f044607bc01895296223a27e4466ab0913\",\n  \"compile_time\" : \"Fri Sep 2 08:19:12 UTC 2016\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"ba19f7d3be65\"\n}\n}\n[]\n\n\n\nThanks to the docker swarm internal routing mesh, you can actually perform the previous query to any node of the swarm, it will be redirected to a node where the request on port 1026 can be attended (i.e, any node running Orion).\n\n\nLet's insert some data...\n\n\n$ sh ../insert.sh $(docker-machine ip ms-worker1)\n\n\n\nAnd check it's there...\n\n\n$ sh ../query.sh $(docker-machine ip ms-worker0)\n...\n[\n    {\n        \"id\": \"Room1\",\n        \"pressure\": {\n            \"metadata\": {},\n            \"type\": \"Integer\",\n            \"value\": 720\n        },\n        \"temperature\": {\n            \"metadata\": {},\n            \"type\": \"Float\",\n            \"value\": 23\n        },\n        \"type\": \"Room\"\n    }\n]\n\n\n\nYes, you can query any of the three nodes.\n\n\nSwarm's internal load balancer will be load-balancing in a round-robin approach all the requests for an orion service among the orion tasks running in the swarm.\n\n\nRescaling Orion\n\n\nScaling up and down orion is a simple as runnnig something like...\n\n\n$ docker service scale orion_orion=2\n\n\n\n(this maps to the \n\"replicas\"\n argument in the docker-compose)\n\n\nConsequently, one of the nodes (ms-worker1 in my case) is no longer running Orion...\n\n\n$ docker service ps orion_orion\nID            NAME                    IMAGE               NODE         DESIRED STATE  CURRENT STATE           ERROR  PORTS\n2tibpye24o5q  orion_orion.2  fiware/orion:1.3.0  ms-manager0  Running        Running 11 minutes ago         \nw9zmn8pp61ql  orion_orion.3  fiware/orion:1.3.0  ms-worker0   Running        Running 11 minutes ago\n\n\n\nBut still responds to the querying as mentioned above...\n\n\n$ sh ../query.sh $(docker-machine ip ms-worker1)\n{\n  \"orion\" : {\n  \"version\" : \"1.3.0\",\n  \"uptime\" : \"0 d, 0 h, 14 m, 30 s\",\n  \"git_hash\" : \"cb6813f044607bc01895296223a27e4466ab0913\",\n  \"compile_time\" : \"Fri Sep 2 08:19:12 UTC 2016\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"ba19f7d3be65\"\n}\n}\n[]\n\n\n\nYou can see the \nmongo replica recipe\n to see how to scale the mongodb backend. But basically, due to the fact that it's a \"global\" service, you can scale it down like shown before. However, scaling it up might require adding a new node to the swarm because there can be only one instance per node.\n\n\nDealing with failures\n\n\nDocker is taking care of the reconciliation of the services in case a container goes down. Let's show this by running the following (always on the manager node):\n\n\n$ docker ps\nCONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES\nabc5e37037f0        fiware/orion@sha256:734c034d078d22f4479e8d08f75b0486ad5a05bfb36b2a1f1ba90ecdba2040a9                 \"/usr/bin/contextB...\"   2 minutes ago       Up 2 minutes        1026/tcp            orion_orion.1.o9ebbardwvzn1gr11pmf61er8\n1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \"python /src/repli...\"   About an hour ago   Up About an hour                        orion-backend_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \"/usr/bin/mongod -...\"   About an hour ago   Up About an hour    27017/tcp           orion-backend_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz\n\n\n\nSuppose orion container goes down...\n\n\n$ docker rm -f abc5e37037f0\n\n\n\nYou will see it gone, but after a while it will automatically come back.\n\n\n$ docker ps\nCONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES\n1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \"python /src/repli...\"   About an hour ago   Up About an hour                        orion-backend_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \"/usr/bin/mongod -...\"   About an hour ago   Up About an hour    27017/tcp           orion-backend_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz\n\n$ docker ps\nCONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS                  PORTS               NAMES\n60ba3f431d9d        fiware/orion@sha256:734c034d078d22f4479e8d08f75b0486ad5a05bfb36b2a1f1ba90ecdba2040a9                 \"/usr/bin/contextB...\"   6 seconds ago       Up Less than a second   1026/tcp            orion_orion.1.uj1gghehb2s1gnoestup2ugs5\n1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \"python /src/repli...\"   About an hour ago   Up About an hour                            orion-backend_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \"/usr/bin/mongod -...\"   About an hour ago   Up About an hour        27017/tcp           orion-backend_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz\n\n\n\nEven if a whole node goes down, the service will remain working because you had both redundant orion instances and redundant db replicas.\n\n\n$ docker-machine rm ms-worker0\n\n\n\nYou will still get replies to...\n\n\n$ sh ../query.sh $(docker-machine ip ms-manager0)\n$ sh ../query.sh $(docker-machine ip ms-worker1)\n\n\n\nNetworks considerations\n\n\nIn this case, all containers are attached to the same overlay network (backend) over which they communicate to each other. However, if you have a different configuration and are running any of the containers behind a firewall, remember to keep traffic open for TCP at ports 1026 (Orion's default) and 27017 (Mongo's default).\n\n\nWhen containers (tasks) of a service are launched, they get assigned an IP address in this overlay network. Other services of your application's architecture should not be relying on these IPs because they may change (for example, due to a dynamic rescheduling). The good think is that docker creates a virtual ip for the service as a whole, so all traffic to this address will be load-balanced to the tasks adresses.\n\n\nThanks to swarms docker internal DNS you can also use the name of the service to connect to. If you look at the \ndocker-compose.yml\n file of this recipe, orion is started with the name of the mongo service as \ndbhost\n param (regardless if it was a single mongo instance of a whole replica-set).\n\n\nHowever, to access the container from outside of the overlay network (for example from the host) you would need to access the ip of the container's interface to the \ndocker_gwbridge\n. It seem there's no easy way to get that information from the outside (see \nthis open issue\n. In the walkthrough, we queried orion through one of the swarm nodes because we rely on docker ingress network routing the traffic all the way to one of the containerized orion services.\n\n\nOpen interesting issues:\n\n\n\n\nhttps://github.com/docker/swarm/issues/1106\n\n\nhttps://github.com/docker/docker/issues/27082\n\n\nhttps://github.com/docker/docker/issues/29816\n\n\nhttps://github.com/docker/docker/issues/26696\n\n\nhttps://github.com/docker/docker/issues/23813\n\n\n\n\nMore info about docker network internals can be read at:\n\n\n\n\nDocker Reference Architecture", 
            "title": "HA"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#orion-in-ha", 
            "text": "This recipe shows how to deploy an scalable  Orion Context Broker  service backed with an scalable  replica set  of MongoDB instances.  All elements will be running in docker containers, defined in docker-compose files. Actually, this recipe focuses on the deployment of the Orion frontend, reusing the  mongodb replica recipe  for its backend.  The final deployment is represented by the following picture:   IMPORTANT:  As explained in the  mongo replica recipe , that recipe is not ready for production deployments for security reasons. Look at the  \"Further Improvements\"  section for more details.", 
            "title": "Orion in HA"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#how-to-use", 
            "text": "Firstly, you need to have a Docker Swarm (docker  = 1.13) already setup. If you don't have one, checkout the  tools  section for a quick way to setup a local swarm.  $ miniswarm start 3\n$ eval $(docker-machine env ms-manager0)  Then, simply run...  $ sh deploy_back.sh  Wait some time until the backend is ready and then...  $ sh deploy_front.sh  At some point, your deployment should look like this...  $ docker service ls\nID            NAME                            MODE        REPLICAS  IMAGE\nnrxbm6k0a2yn  orion-backend_mongo             global      3/3       mongo:3.2\nrgws8vumqye2  orion-backend_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\nzk7nu592vsde  orion_orion                     replicated  3/3       fiware/orion:1.3.0  As shown above, if you see  3/3  in the replicas column it means the 3 replicas are up and running.", 
            "title": "How to use"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#a-walkthrough", 
            "text": "You can check the distribution of the containers of a service (a.k.a tasks) through the swarm running the following...  $ docker service ps orion_orion\nID            NAME                    IMAGE               NODE         DESIRED STATE  CURRENT STATE               ERROR  PORTS\nwwgt3q6nqqg3  orion_orion.1  fiware/orion:1.3.0  ms-worker0   Running        Running 9 minutes ago          \nl1wavgqra8ry  orion_orion.2  fiware/orion:1.3.0  ms-worker1   Running        Running 9 minutes ago          \nz20v0pnym8ky  orion_orion.3  fiware/orion:1.3.0  ms-manager0  Running        Running 25 minutes ago  The good news is that, as you can see from the above output, by default docker already took care of deploying all the replicas of the service  context-broker_orion  to different hosts.  Of course, with the use of labels, constraints or deploying mode you have the power to customize the distribution of tasks among swarm nodes. You can see the  mongo replica recipe  to understand the deployment of the  mongo-replica_mongo  service.  Now, let's query Orion to check it's truly up and running. The question now is... where is Orion actually running? We'll cover the network internals later, but for now let's query the manager node...  $ sh ../query.sh $(docker-machine ip ms-manager0)  You will get something like...  {\n  \"orion\" : {\n  \"version\" : \"1.3.0\",\n  \"uptime\" : \"0 d, 0 h, 18 m, 13 s\",\n  \"git_hash\" : \"cb6813f044607bc01895296223a27e4466ab0913\",\n  \"compile_time\" : \"Fri Sep 2 08:19:12 UTC 2016\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"ba19f7d3be65\"\n}\n}\n[]  Thanks to the docker swarm internal routing mesh, you can actually perform the previous query to any node of the swarm, it will be redirected to a node where the request on port 1026 can be attended (i.e, any node running Orion).  Let's insert some data...  $ sh ../insert.sh $(docker-machine ip ms-worker1)  And check it's there...  $ sh ../query.sh $(docker-machine ip ms-worker0)\n...\n[\n    {\n        \"id\": \"Room1\",\n        \"pressure\": {\n            \"metadata\": {},\n            \"type\": \"Integer\",\n            \"value\": 720\n        },\n        \"temperature\": {\n            \"metadata\": {},\n            \"type\": \"Float\",\n            \"value\": 23\n        },\n        \"type\": \"Room\"\n    }\n]  Yes, you can query any of the three nodes.  Swarm's internal load balancer will be load-balancing in a round-robin approach all the requests for an orion service among the orion tasks running in the swarm.", 
            "title": "A walkthrough"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#rescaling-orion", 
            "text": "Scaling up and down orion is a simple as runnnig something like...  $ docker service scale orion_orion=2  (this maps to the  \"replicas\"  argument in the docker-compose)  Consequently, one of the nodes (ms-worker1 in my case) is no longer running Orion...  $ docker service ps orion_orion\nID            NAME                    IMAGE               NODE         DESIRED STATE  CURRENT STATE           ERROR  PORTS\n2tibpye24o5q  orion_orion.2  fiware/orion:1.3.0  ms-manager0  Running        Running 11 minutes ago         \nw9zmn8pp61ql  orion_orion.3  fiware/orion:1.3.0  ms-worker0   Running        Running 11 minutes ago  But still responds to the querying as mentioned above...  $ sh ../query.sh $(docker-machine ip ms-worker1)\n{\n  \"orion\" : {\n  \"version\" : \"1.3.0\",\n  \"uptime\" : \"0 d, 0 h, 14 m, 30 s\",\n  \"git_hash\" : \"cb6813f044607bc01895296223a27e4466ab0913\",\n  \"compile_time\" : \"Fri Sep 2 08:19:12 UTC 2016\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"ba19f7d3be65\"\n}\n}\n[]  You can see the  mongo replica recipe  to see how to scale the mongodb backend. But basically, due to the fact that it's a \"global\" service, you can scale it down like shown before. However, scaling it up might require adding a new node to the swarm because there can be only one instance per node.", 
            "title": "Rescaling Orion"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#dealing-with-failures", 
            "text": "Docker is taking care of the reconciliation of the services in case a container goes down. Let's show this by running the following (always on the manager node):  $ docker ps\nCONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES\nabc5e37037f0        fiware/orion@sha256:734c034d078d22f4479e8d08f75b0486ad5a05bfb36b2a1f1ba90ecdba2040a9                 \"/usr/bin/contextB...\"   2 minutes ago       Up 2 minutes        1026/tcp            orion_orion.1.o9ebbardwvzn1gr11pmf61er8\n1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \"python /src/repli...\"   About an hour ago   Up About an hour                        orion-backend_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \"/usr/bin/mongod -...\"   About an hour ago   Up About an hour    27017/tcp           orion-backend_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz  Suppose orion container goes down...  $ docker rm -f abc5e37037f0  You will see it gone, but after a while it will automatically come back.  $ docker ps\nCONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES\n1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \"python /src/repli...\"   About an hour ago   Up About an hour                        orion-backend_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \"/usr/bin/mongod -...\"   About an hour ago   Up About an hour    27017/tcp           orion-backend_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz\n\n$ docker ps\nCONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS                  PORTS               NAMES\n60ba3f431d9d        fiware/orion@sha256:734c034d078d22f4479e8d08f75b0486ad5a05bfb36b2a1f1ba90ecdba2040a9                 \"/usr/bin/contextB...\"   6 seconds ago       Up Less than a second   1026/tcp            orion_orion.1.uj1gghehb2s1gnoestup2ugs5\n1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \"python /src/repli...\"   About an hour ago   Up About an hour                            orion-backend_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \"/usr/bin/mongod -...\"   About an hour ago   Up About an hour        27017/tcp           orion-backend_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz  Even if a whole node goes down, the service will remain working because you had both redundant orion instances and redundant db replicas.  $ docker-machine rm ms-worker0  You will still get replies to...  $ sh ../query.sh $(docker-machine ip ms-manager0)\n$ sh ../query.sh $(docker-machine ip ms-worker1)", 
            "title": "Dealing with failures"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#networks-considerations", 
            "text": "In this case, all containers are attached to the same overlay network (backend) over which they communicate to each other. However, if you have a different configuration and are running any of the containers behind a firewall, remember to keep traffic open for TCP at ports 1026 (Orion's default) and 27017 (Mongo's default).  When containers (tasks) of a service are launched, they get assigned an IP address in this overlay network. Other services of your application's architecture should not be relying on these IPs because they may change (for example, due to a dynamic rescheduling). The good think is that docker creates a virtual ip for the service as a whole, so all traffic to this address will be load-balanced to the tasks adresses.  Thanks to swarms docker internal DNS you can also use the name of the service to connect to. If you look at the  docker-compose.yml  file of this recipe, orion is started with the name of the mongo service as  dbhost  param (regardless if it was a single mongo instance of a whole replica-set).  However, to access the container from outside of the overlay network (for example from the host) you would need to access the ip of the container's interface to the  docker_gwbridge . It seem there's no easy way to get that information from the outside (see  this open issue . In the walkthrough, we queried orion through one of the swarm nodes because we rely on docker ingress network routing the traffic all the way to one of the containerized orion services.", 
            "title": "Networks considerations"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#open-interesting-issues", 
            "text": "https://github.com/docker/swarm/issues/1106  https://github.com/docker/docker/issues/27082  https://github.com/docker/docker/issues/29816  https://github.com/docker/docker/issues/26696  https://github.com/docker/docker/issues/23813   More info about docker network internals can be read at:   Docker Reference Architecture", 
            "title": "Open interesting issues:"
        }, 
        {
            "location": "/data-management/sth/readme/", 
            "text": "Comet\n\n\nHere you can find recipes aimed at different usages of Comet, the Reference Implementation of the STH Generic Enabler. We assume you are already familiar with Comet. If not, refer to the \nofficial documentation\n.\n\n\nThe easiest and simplest way to try Comet is to follow the \nStandalone Walkthrough\n.\n\n\nInstructions on how to prepare your environment to test these recipes are given in \nhttps://github.com/martel-innovate/smartsdk-recipes\n.", 
            "title": "Intro"
        }, 
        {
            "location": "/data-management/sth/readme/#comet", 
            "text": "Here you can find recipes aimed at different usages of Comet, the Reference Implementation of the STH Generic Enabler. We assume you are already familiar with Comet. If not, refer to the  official documentation .  The easiest and simplest way to try Comet is to follow the  Standalone Walkthrough .  Instructions on how to prepare your environment to test these recipes are given in  https://github.com/martel-innovate/smartsdk-recipes .", 
            "title": "Comet"
        }, 
        {
            "location": "/data-management/sth/standalone/readme/", 
            "text": "Standalone\n\n\nIntroduction\n\n\nThe idea of this standalone walkthrough is to test and showcase the Comet Generic Enabler within a simple notification-based scenario, like the one illustrated below.\n\n\n\n\nA walkthrough\n\n\nFirstly, you need to have a Docker Swarm (docker \n= 1.13) already setup. If you don't have one, checkout the \ntools\n section for a quick way to setup a local swarm.\n\n\nminiswarm start 3\neval $(docker-machine env ms-manager0)\n\n\n\nTo start the whole stack simply run, as usual:\n\n\ndocker stack deploy -c docker-compose.yml comet\n\n\n\nThen, wait until you see all the replicas up and running:\n\n\ndocker service ls\nID            NAME               MODE        REPLICAS  IMAGE\n1ysxmrxrqvp4  comet_comet-mongo  replicated  1/1       mongo:3.2\n8s9acybjxo0m  comet_orion        replicated  1/1       fiware/orion:latest\nra84eex0zsd0  comet_comet        replicated  3/3       telefonicaiot/fiware-sth-comet:latest\nxg8ds3szkoi7  comet_orion-mongo  replicated  1/1       mongo:3.2\n\n\n\nNow let's start some checkups. For convenience, let's save the IP address of the Orion and Comet services. In this scenario, since both are deployed on Swarm exposing their services ports, only one entry-point to the Swarm's ingress network will suffice.\n\n\nORION=http://$(docker-machine ip ms-manager0)\nCOMET=http://$(docker-machine ip ms-manager0)\n\n\n\nLet's start some checkups, first making sure Orion is up and running.\n\n\nsh ../../context-broker/query.sh $ORION\n{\n\"orion\" : {\n  \"version\" : \"1.7.0-next\",\n  \"uptime\" : \"0 d, 0 h, 1 m, 39 s\",\n  \"git_hash\" : \"f710ee525f0fa55f665e578e309fc716c12cfd99\",\n  \"compile_time\" : \"Wed Feb 22 10:14:18 UTC 2017\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"b99744612d0b\"\n}\n}\n[]\n\n\n\nLet's insert some simple data (Room1 measurements):\n\n\nsh ../../context-broker/insert.sh $ORION\n\n\n\nNow, let's subscribe Comet to the notifications of changes in temperature of Room1.\n\n\nsh ../subscribe.sh $COMET\n{\n  \"subscribeResponse\" : {\n    \"subscriptionId\" : \"58b98c0cdb69948641065907\",\n    \"duration\" : \"PT24H\"\n  }\n}\n\n\n\nLet's update the temperature value in Orion...\n\n\nsh ../../context-broker/update.sh $ORION\n\n\n\nAnd check you can see the Short-Term-Historical view of both measurements.\n\n\nsh ../query_sth.sh $COMET\n{\n    \"contextResponses\": [\n        {\n            \"contextElement\": {\n                \"attributes\": [\n                    {\n                        \"name\": \"temperature\",\n                        \"values\": [\n                            {\n                                \"attrType\": \"Float\",\n                                \"attrValue\": 23,\n                                \"recvTime\": \"2017-03-03T15:30:20.650Z\"\n                            },\n                            {\n                                \"attrType\": \"Float\",\n                                \"attrValue\": 29.3,\n                                \"recvTime\": \"2017-03-03T15:32:48.741Z\"\n                            }\n                        ]\n                    }\n                ],\n                \"id\": \"Room1\",\n                \"isPattern\": false,\n                \"type\": \"Room\"\n            },\n            \"statusCode\": {\n                \"code\": \"200\",\n                \"reasonPhrase\": \"OK\"\n            }\n        }\n    ]\n}", 
            "title": "Standalone"
        }, 
        {
            "location": "/data-management/sth/standalone/readme/#standalone", 
            "text": "", 
            "title": "Standalone"
        }, 
        {
            "location": "/data-management/sth/standalone/readme/#introduction", 
            "text": "The idea of this standalone walkthrough is to test and showcase the Comet Generic Enabler within a simple notification-based scenario, like the one illustrated below.", 
            "title": "Introduction"
        }, 
        {
            "location": "/data-management/sth/standalone/readme/#a-walkthrough", 
            "text": "Firstly, you need to have a Docker Swarm (docker  = 1.13) already setup. If you don't have one, checkout the  tools  section for a quick way to setup a local swarm.  miniswarm start 3\neval $(docker-machine env ms-manager0)  To start the whole stack simply run, as usual:  docker stack deploy -c docker-compose.yml comet  Then, wait until you see all the replicas up and running:  docker service ls\nID            NAME               MODE        REPLICAS  IMAGE\n1ysxmrxrqvp4  comet_comet-mongo  replicated  1/1       mongo:3.2\n8s9acybjxo0m  comet_orion        replicated  1/1       fiware/orion:latest\nra84eex0zsd0  comet_comet        replicated  3/3       telefonicaiot/fiware-sth-comet:latest\nxg8ds3szkoi7  comet_orion-mongo  replicated  1/1       mongo:3.2  Now let's start some checkups. For convenience, let's save the IP address of the Orion and Comet services. In this scenario, since both are deployed on Swarm exposing their services ports, only one entry-point to the Swarm's ingress network will suffice.  ORION=http://$(docker-machine ip ms-manager0)\nCOMET=http://$(docker-machine ip ms-manager0)  Let's start some checkups, first making sure Orion is up and running.  sh ../../context-broker/query.sh $ORION\n{\n\"orion\" : {\n  \"version\" : \"1.7.0-next\",\n  \"uptime\" : \"0 d, 0 h, 1 m, 39 s\",\n  \"git_hash\" : \"f710ee525f0fa55f665e578e309fc716c12cfd99\",\n  \"compile_time\" : \"Wed Feb 22 10:14:18 UTC 2017\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"b99744612d0b\"\n}\n}\n[]  Let's insert some simple data (Room1 measurements):  sh ../../context-broker/insert.sh $ORION  Now, let's subscribe Comet to the notifications of changes in temperature of Room1.  sh ../subscribe.sh $COMET\n{\n  \"subscribeResponse\" : {\n    \"subscriptionId\" : \"58b98c0cdb69948641065907\",\n    \"duration\" : \"PT24H\"\n  }\n}  Let's update the temperature value in Orion...  sh ../../context-broker/update.sh $ORION  And check you can see the Short-Term-Historical view of both measurements.  sh ../query_sth.sh $COMET\n{\n    \"contextResponses\": [\n        {\n            \"contextElement\": {\n                \"attributes\": [\n                    {\n                        \"name\": \"temperature\",\n                        \"values\": [\n                            {\n                                \"attrType\": \"Float\",\n                                \"attrValue\": 23,\n                                \"recvTime\": \"2017-03-03T15:30:20.650Z\"\n                            },\n                            {\n                                \"attrType\": \"Float\",\n                                \"attrValue\": 29.3,\n                                \"recvTime\": \"2017-03-03T15:32:48.741Z\"\n                            }\n                        ]\n                    }\n                ],\n                \"id\": \"Room1\",\n                \"isPattern\": false,\n                \"type\": \"Room\"\n            },\n            \"statusCode\": {\n                \"code\": \"200\",\n                \"reasonPhrase\": \"OK\"\n            }\n        }\n    ]\n}", 
            "title": "A walkthrough"
        }, 
        {
            "location": "/data-management/sth/ha/readme/", 
            "text": "HA\n\n\nIntroduction\n\n\nLet's test a deployment of Comet with multiple replicas in both its front-end and backend. The idea now is to get the scenario illustrated below.\n\n\n\n\nLater, this could be combined for example with an \nHA deployment of Orion Context Broker\n.\n\n\nA walkthrough\n\n\nFirst, you need to have a Docker Swarm (docker \n= 1.13) already setup. If you don't have one, checkout the \ntools\n section for a quick way to setup a local swarm.\n\n\nminiswarm start 3\neval $(docker-machine env ms-manager0)\n\n\n\nTo start the comet's backend simply run...\n\n\nsh deploy_back.sh\n\n\n\nAfter a while, when the replica-set is ready, you can deploy comet by running...\n\n\nsh deploy_front.sh\n\n\n\nNow, as usual, a brief test to confirm everything is properly connected. As a source of notifications, we have deployed Orion in the swarm (see \nOrion in HA\n for example).\n\n\nFor convenience, let's save the IP address of the Orion and Comet services. In this scenario, since both are deployed on Swarm exposing their services ports, only one entry-point to the Swarm's ingress network will suffice.\n\n\nORION=http://$(docker-machine ip ms-manager0)\nCOMET=http://$(docker-machine ip ms-manager0)\n\n\n\nInsert:\n\n\nsh ../../context-broker/insert.sh $ORION\nsh ../../context-broker/query.sh $ORION\n...\n\n\n\nSubscribe:\n\n\nsh ../subscribe.sh $ORION\n{\n  \"subscribeResponse\" : {\n    \"subscriptionId\" : \"58bd1940b97cc713f5eacdb7\",\n    \"duration\" : \"PT24H\"\n  }\n}\n\n\n\nUpdate:\n\n\nsh ../../context-broker/update.sh $ORION\n\n\n\nAnd voila:\n\n\nsh ../query_sth.sh $COMET\n{\n\"contextResponses\": [\n    {\n        \"contextElement\": {\n            \"attributes\": [\n                {\n                    \"name\": \"temperature\",\n                    \"values\": [\n                        {\n                            \"attrType\": \"Float\",\n                            \"attrValue\": 23,\n                            \"recvTime\": \"2017-03-06T08:09:36.493Z\"\n                        },\n                        {\n                            \"attrType\": \"Float\",\n                            \"attrValue\": 29.3,\n                            \"recvTime\": \"2017-03-06T08:11:14.044Z\"\n                        }\n                    ]\n                }\n            ],\n            \"id\": \"Room1\",\n            \"isPattern\": false,\n            \"type\": \"Room\"\n        },\n        \"statusCode\": {\n            \"code\": \"200\",\n            \"reasonPhrase\": \"OK\"\n        }\n    }\n]\n}", 
            "title": "HA"
        }, 
        {
            "location": "/data-management/sth/ha/readme/#ha", 
            "text": "", 
            "title": "HA"
        }, 
        {
            "location": "/data-management/sth/ha/readme/#introduction", 
            "text": "Let's test a deployment of Comet with multiple replicas in both its front-end and backend. The idea now is to get the scenario illustrated below.   Later, this could be combined for example with an  HA deployment of Orion Context Broker .", 
            "title": "Introduction"
        }, 
        {
            "location": "/data-management/sth/ha/readme/#a-walkthrough", 
            "text": "First, you need to have a Docker Swarm (docker  = 1.13) already setup. If you don't have one, checkout the  tools  section for a quick way to setup a local swarm.  miniswarm start 3\neval $(docker-machine env ms-manager0)  To start the comet's backend simply run...  sh deploy_back.sh  After a while, when the replica-set is ready, you can deploy comet by running...  sh deploy_front.sh  Now, as usual, a brief test to confirm everything is properly connected. As a source of notifications, we have deployed Orion in the swarm (see  Orion in HA  for example).  For convenience, let's save the IP address of the Orion and Comet services. In this scenario, since both are deployed on Swarm exposing their services ports, only one entry-point to the Swarm's ingress network will suffice.  ORION=http://$(docker-machine ip ms-manager0)\nCOMET=http://$(docker-machine ip ms-manager0)  Insert:  sh ../../context-broker/insert.sh $ORION\nsh ../../context-broker/query.sh $ORION\n...  Subscribe:  sh ../subscribe.sh $ORION\n{\n  \"subscribeResponse\" : {\n    \"subscriptionId\" : \"58bd1940b97cc713f5eacdb7\",\n    \"duration\" : \"PT24H\"\n  }\n}  Update:  sh ../../context-broker/update.sh $ORION  And voila:  sh ../query_sth.sh $COMET\n{\n\"contextResponses\": [\n    {\n        \"contextElement\": {\n            \"attributes\": [\n                {\n                    \"name\": \"temperature\",\n                    \"values\": [\n                        {\n                            \"attrType\": \"Float\",\n                            \"attrValue\": 23,\n                            \"recvTime\": \"2017-03-06T08:09:36.493Z\"\n                        },\n                        {\n                            \"attrType\": \"Float\",\n                            \"attrValue\": 29.3,\n                            \"recvTime\": \"2017-03-06T08:11:14.044Z\"\n                        }\n                    ]\n                }\n            ],\n            \"id\": \"Room1\",\n            \"isPattern\": false,\n            \"type\": \"Room\"\n        },\n        \"statusCode\": {\n            \"code\": \"200\",\n            \"reasonPhrase\": \"OK\"\n        }\n    }\n]\n}", 
            "title": "A walkthrough"
        }, 
        {
            "location": "/iot-services/readme/", 
            "text": "Comming soon...", 
            "title": "IoT Broker"
        }, 
        {
            "location": "/utils/mongodb/replica/readme/", 
            "text": "MongoDB Replica Set\n\n\nThis recipe aims to deploy and control a \nreplica set\n of MongoDB instances in a Docker Swarm.\n\n\n\n\nIMPORTANT:\n This recipe is not yet ready for production environments. See \nfurther improvements\n section for more details.\n\n\nHow to use\n\n\nFirstly, you need to have a Docker Swarm (docker \n= 1.13) already setup. If you don't have one, checkout the \ntools\n section for a quick way to setup a local swarm.\n\n\nminiswarm start 3\neval $(docker-machine env ms-manager0)\n\n\n\nThen, simply run...\n\n\nsh deploy.sh\n\n\n\nAllow some time while images are pulled in the nodes and services are deployed. After a couple of minutes, you can check if all services are up, as usual, running...\n\n\n$ docker service ls\nID            NAME                            MODE        REPLICAS  IMAGE\nfjxof1n5ce58  mongo-replica_mongo             global      3/3       mongo:latest\nyzsur7rb4mg1  mongo-replica_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\n\n\n\nA Walkthrough\n\n\nAs shown before, the recipe consists of basically two services, namely, one for mongo instances and one for controlling the replica-set.\n\n\nThe mongo service is deployed in \"global\" mode, meaning that docker will run one instance of mongod per swarm node in the cluster.\n\n\nAt the swarm's master node, a python-based controller script will be deployed to configure and maintain the mongodb replica-set.\n\n\nLet's now check that the controller worked fine inspecting the logs of the mongo-replica_mongo-controller service. This can be done with either...\n\n\n$ docker service logs mongo-replica_mongo-controller\n\n\n\nor running the following...\n\n\n$  docker logs $(docker ps -f \"name=mongo-replica_mongo-controller\" -q)\nINFO:__main__:Waiting some time before starting\nINFO:__main__:Initial config: {'version': 1, '_id': 'rs', 'members': [{'_id': 0, 'host': '10.0.0.5:27017'}, {'_id': 1, 'host': '10.0.0.3:27017'}, {'_id': 2, 'host': '10.0.0.4:27017'}]}\nINFO:__main__:replSetInitiate: {'ok': 1.0}\n\n\n\nAs you can see, the replica-set was configured with 3 replicas represented by containers running in the same overlay network. You can also run a mongo command in any of the mongo containers and execute \nrs.status()\n to see the same results.\n\n\n$ docker exec -ti d56d17c40f8f mongo\nrs:SECONDARY\n rs.status()\n\n\n\nRescaling the replica-set\n\n\nLet's add a new node to the swarm to see how docker deploys a new task of the mongo service and the controller automatically adds it to the replica-set.\n\n\n# First get the token to join the swarm\n$ docker swarm join-token worker\n\n# Create the new node\n$ docker-machine create -d virtualbox ms-worker2\n$ docker-machine ssh ms-worker2\n\ndocker@ms-worker2:~$ docker swarm join \\\n--token INSERT_TOKEN_HERE \\\n192.168.99.100:2377\n\ndocker@ms-worker2:~$ exit\n\n\n\nBack to the host, some minutes later...\n\n\n$ docker service ls\nID            NAME                            MODE        REPLICAS  IMAGE\nfjxof1n5ce58  mongo-replica_mongo             global      4/4       mongo:latest\nyzsur7rb4mg1  mongo-replica_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\n\n$ docker logs $(docker ps -f \"name=mongo-replica_mongo-controller\" -q)\n...\nINFO:__main__:To add: {'10.0.0.8'}\nINFO:__main__:New config: {'version': 2, '_id': 'rs', 'members': [{'_id': 0, 'host': '10.0.0.5:27017'}, {'_id': 1, 'host': '10.0.0.3:27017'}, {'_id': 2, 'host': '10.0.0.4:27017'}, {'_id': 3, 'host': '10.0.0.8:27017'}]}\nINFO:__main__:replSetReconfig: {'ok': 1.0}\n\n\n\nIf a node goes down, the replica-set will be automatically reconfigured at the application level by mongo. Docker, on the other hand, will not reschedule the replica because it's expected to run one only one per node.\n\n\nNOTE\n: If you don't want to have a replica in every node of the swarm, the solution for now is using a combination of constraints and node tags. You can read more about this in \nthis Github issue\n.\n\n\nFor further details, refer to the \ndocker-compose.yml\n file or \nreplica_ctrl.py\n.\n\n\nChallenges and Further improvements\n\n\nThe main challenge for this script was to know at runtime where each mongod instance was running so as to configure the replica-set properly. The idea of the new orchestration features in swarm is that you really shouldn't care where they run as long as swarm keeps them up and running. But mongo needs to know that in order to configure the replica set.\n\n\nSo the first approach is to find out this information from the docker api. Also, since the recipe is expected to be self-contained and work without dependencies on things running outside the swarm, we need to get this information from a container running in the swarm. My understanding is that such an introspective api to safely retrieve this kind of information is yet to come (e.g \nthis issue\n and related ones such as \nthis one\n). So for now this is depending on access to the host's docker socket \n(terribly insecure workaround)\n. A different approach to explore would be passing to the script at runtime the list of IPs. IPs of the containers if possible, or if not, IPs of the nodes where the services are deployed and use different ports in each replica member so as to avoid clashes in the docker ingress network. Or, something considering a custom IPM via plugins.\n\n\nFurther things to keep in mind:\n\n\n\n\nThe script has some improvement suggestions marked with TODO comments.\n\n\nAt the moment this recipe does not include a data persistence solution.\n\n\nConsider using authentication in the replica-set.", 
            "title": "Replica Set"
        }, 
        {
            "location": "/utils/mongodb/replica/readme/#mongodb-replica-set", 
            "text": "This recipe aims to deploy and control a  replica set  of MongoDB instances in a Docker Swarm.   IMPORTANT:  This recipe is not yet ready for production environments. See  further improvements  section for more details.", 
            "title": "MongoDB Replica Set"
        }, 
        {
            "location": "/utils/mongodb/replica/readme/#how-to-use", 
            "text": "Firstly, you need to have a Docker Swarm (docker  = 1.13) already setup. If you don't have one, checkout the  tools  section for a quick way to setup a local swarm.  miniswarm start 3\neval $(docker-machine env ms-manager0)  Then, simply run...  sh deploy.sh  Allow some time while images are pulled in the nodes and services are deployed. After a couple of minutes, you can check if all services are up, as usual, running...  $ docker service ls\nID            NAME                            MODE        REPLICAS  IMAGE\nfjxof1n5ce58  mongo-replica_mongo             global      3/3       mongo:latest\nyzsur7rb4mg1  mongo-replica_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest", 
            "title": "How to use"
        }, 
        {
            "location": "/utils/mongodb/replica/readme/#a-walkthrough", 
            "text": "As shown before, the recipe consists of basically two services, namely, one for mongo instances and one for controlling the replica-set.  The mongo service is deployed in \"global\" mode, meaning that docker will run one instance of mongod per swarm node in the cluster.  At the swarm's master node, a python-based controller script will be deployed to configure and maintain the mongodb replica-set.  Let's now check that the controller worked fine inspecting the logs of the mongo-replica_mongo-controller service. This can be done with either...  $ docker service logs mongo-replica_mongo-controller  or running the following...  $  docker logs $(docker ps -f \"name=mongo-replica_mongo-controller\" -q)\nINFO:__main__:Waiting some time before starting\nINFO:__main__:Initial config: {'version': 1, '_id': 'rs', 'members': [{'_id': 0, 'host': '10.0.0.5:27017'}, {'_id': 1, 'host': '10.0.0.3:27017'}, {'_id': 2, 'host': '10.0.0.4:27017'}]}\nINFO:__main__:replSetInitiate: {'ok': 1.0}  As you can see, the replica-set was configured with 3 replicas represented by containers running in the same overlay network. You can also run a mongo command in any of the mongo containers and execute  rs.status()  to see the same results.  $ docker exec -ti d56d17c40f8f mongo\nrs:SECONDARY  rs.status()", 
            "title": "A Walkthrough"
        }, 
        {
            "location": "/utils/mongodb/replica/readme/#rescaling-the-replica-set", 
            "text": "Let's add a new node to the swarm to see how docker deploys a new task of the mongo service and the controller automatically adds it to the replica-set.  # First get the token to join the swarm\n$ docker swarm join-token worker\n\n# Create the new node\n$ docker-machine create -d virtualbox ms-worker2\n$ docker-machine ssh ms-worker2\n\ndocker@ms-worker2:~$ docker swarm join \\\n--token INSERT_TOKEN_HERE \\\n192.168.99.100:2377\n\ndocker@ms-worker2:~$ exit  Back to the host, some minutes later...  $ docker service ls\nID            NAME                            MODE        REPLICAS  IMAGE\nfjxof1n5ce58  mongo-replica_mongo             global      4/4       mongo:latest\nyzsur7rb4mg1  mongo-replica_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\n\n$ docker logs $(docker ps -f \"name=mongo-replica_mongo-controller\" -q)\n...\nINFO:__main__:To add: {'10.0.0.8'}\nINFO:__main__:New config: {'version': 2, '_id': 'rs', 'members': [{'_id': 0, 'host': '10.0.0.5:27017'}, {'_id': 1, 'host': '10.0.0.3:27017'}, {'_id': 2, 'host': '10.0.0.4:27017'}, {'_id': 3, 'host': '10.0.0.8:27017'}]}\nINFO:__main__:replSetReconfig: {'ok': 1.0}  If a node goes down, the replica-set will be automatically reconfigured at the application level by mongo. Docker, on the other hand, will not reschedule the replica because it's expected to run one only one per node.  NOTE : If you don't want to have a replica in every node of the swarm, the solution for now is using a combination of constraints and node tags. You can read more about this in  this Github issue .  For further details, refer to the  docker-compose.yml  file or  replica_ctrl.py .", 
            "title": "Rescaling the replica-set"
        }, 
        {
            "location": "/utils/mongodb/replica/readme/#challenges-and-further-improvements", 
            "text": "The main challenge for this script was to know at runtime where each mongod instance was running so as to configure the replica-set properly. The idea of the new orchestration features in swarm is that you really shouldn't care where they run as long as swarm keeps them up and running. But mongo needs to know that in order to configure the replica set.  So the first approach is to find out this information from the docker api. Also, since the recipe is expected to be self-contained and work without dependencies on things running outside the swarm, we need to get this information from a container running in the swarm. My understanding is that such an introspective api to safely retrieve this kind of information is yet to come (e.g  this issue  and related ones such as  this one ). So for now this is depending on access to the host's docker socket  (terribly insecure workaround) . A different approach to explore would be passing to the script at runtime the list of IPs. IPs of the containers if possible, or if not, IPs of the nodes where the services are deployed and use different ports in each replica member so as to avoid clashes in the docker ingress network. Or, something considering a custom IPM via plugins.  Further things to keep in mind:   The script has some improvement suggestions marked with TODO comments.  At the moment this recipe does not include a data persistence solution.  Consider using authentication in the replica-set.", 
            "title": "Challenges and Further improvements"
        }, 
        {
            "location": "/tools/readme/", 
            "text": "Tools\n\n\nThis section contains useful (and sometimes temporary) scripts as well as references to tools, projects and pieces of documentation used for the development of the recipes.\n\n\nThe basic environment setup is explained in landing page of the repository: \nhttps://github.com/martel-innovate/smartsdk-recipes\n.\n\n\nEnv-related\n\n\n\n\n\n\nminiswarm\n\n\nTo quickly test deployments on a local swarm. Refer to the \nrepo\n for further details, but getting started is as simple as:\n\n\n# First-time only to install miniswarm\n$ curl -sSL https://raw.githubusercontent.com/aelsabbahy/miniswarm/master/miniswarm -o /usr/local/bin/miniswarm\n$ chmod +rx /usr/local/bin/miniswarm\n\n# Every time you create/destroy a swarm\n$ miniswarm start 3\n$ miniswarm delete\n\n\n\n\n\n\n\nwait-for-it\n\n\nUseful shell script used when you need to wait for a service to be started.\n\n\nNote: This might no longer be needed since docker introduced the \nhealthchecks\n feature.\n\n\n\n\n\n\nportainer\n\n\nIf you'd like an UI with info about your swarm:\n\n\ndocker service create \\\n--name portainer \\\n--publish 9000:9000 \\\n--constraint 'node.role == manager' \\\n--mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\\nportainer/portainer \\\n-H unix:///var/run/docker.sock\n\n\n\n\n\n\n\nDocs-related\n\n\n\n\n\n\ngravizo\n\n\nWe use this tool in the docs for \nsimple diagrams\n. Problems with formatting? try the \nconverter\n.\n\n\n\n\n\n\ndraw.io\n\n\nUse this tool when the diagrams start getting too complex of when you foresee the diagram will be complex from the scratch.\n\n\nComplex in the sense that making a simple change takes more time understanding the \n.dot\n than making a manual gui-based change.\n\n\nWhen using draw.io, keep the source file in the repository under a /doc subfolder of the corresponding recipe.\n\n\n\n\n\n\ncolor names\n\n\nThe reference for color names used in \n.dot\n files.\n\n\n\n\n\n\nExperimental\n\n\n\n\n\n\ndiagramr\n\nTo give more docker-related details we could use this tool to create diagrams from docker-compose files. The tools gives also the .dot file, which would be eventually customized and then turned into a png file using \ngraphviz\n.\n$ dot compose.dot -Tpng -o compose.png", 
            "title": "Tools"
        }, 
        {
            "location": "/tools/readme/#tools", 
            "text": "This section contains useful (and sometimes temporary) scripts as well as references to tools, projects and pieces of documentation used for the development of the recipes.  The basic environment setup is explained in landing page of the repository:  https://github.com/martel-innovate/smartsdk-recipes .", 
            "title": "Tools"
        }, 
        {
            "location": "/tools/readme/#env-related", 
            "text": "", 
            "title": "Env-related"
        }, 
        {
            "location": "/tools/readme/#miniswarm", 
            "text": "To quickly test deployments on a local swarm. Refer to the  repo  for further details, but getting started is as simple as:  # First-time only to install miniswarm\n$ curl -sSL https://raw.githubusercontent.com/aelsabbahy/miniswarm/master/miniswarm -o /usr/local/bin/miniswarm\n$ chmod +rx /usr/local/bin/miniswarm\n\n# Every time you create/destroy a swarm\n$ miniswarm start 3\n$ miniswarm delete", 
            "title": "miniswarm"
        }, 
        {
            "location": "/tools/readme/#wait-for-it", 
            "text": "Useful shell script used when you need to wait for a service to be started.  Note: This might no longer be needed since docker introduced the  healthchecks  feature.", 
            "title": "wait-for-it"
        }, 
        {
            "location": "/tools/readme/#portainer", 
            "text": "If you'd like an UI with info about your swarm:  docker service create \\\n--name portainer \\\n--publish 9000:9000 \\\n--constraint 'node.role == manager' \\\n--mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\\nportainer/portainer \\\n-H unix:///var/run/docker.sock", 
            "title": "portainer"
        }, 
        {
            "location": "/tools/readme/#docs-related", 
            "text": "", 
            "title": "Docs-related"
        }, 
        {
            "location": "/tools/readme/#gravizo", 
            "text": "We use this tool in the docs for  simple diagrams . Problems with formatting? try the  converter .", 
            "title": "gravizo"
        }, 
        {
            "location": "/tools/readme/#drawio", 
            "text": "Use this tool when the diagrams start getting too complex of when you foresee the diagram will be complex from the scratch.  Complex in the sense that making a simple change takes more time understanding the  .dot  than making a manual gui-based change.  When using draw.io, keep the source file in the repository under a /doc subfolder of the corresponding recipe.", 
            "title": "draw.io"
        }, 
        {
            "location": "/tools/readme/#color-names", 
            "text": "The reference for color names used in  .dot  files.", 
            "title": "color names"
        }, 
        {
            "location": "/tools/readme/#experimental", 
            "text": "", 
            "title": "Experimental"
        }, 
        {
            "location": "/tools/readme/#diagramr", 
            "text": "To give more docker-related details we could use this tool to create diagrams from docker-compose files. The tools gives also the .dot file, which would be eventually customized and then turned into a png file using  graphviz . $ dot compose.dot -Tpng -o compose.png", 
            "title": "diagramr"
        }
    ]
}